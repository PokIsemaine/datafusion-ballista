import duckdb
import os
import multiprocessing
import pyarrow.parquet as pq
import pyarrow as pa
import psutil


# å†™å…¥åˆ†ç‰‡çš„å‡½æ•°ï¼šå°†ä¸€å®šæ•°é‡çš„è¡Œå†™å…¥ä¸€ä¸ªæ–°çš„ Parquet æ–‡ä»¶
def write_partition_with_limit(input_file, output_file, rows_limit):
    """
    å°†è¾“å…¥çš„ Parquet æ–‡ä»¶åˆ‡åˆ†ä¸ºå°å—ï¼Œæ¯å—åŒ…å« rows_limit è¡Œæ•°æ®ï¼Œå†™å…¥åˆ° output_file ä¸­ã€‚
    """
    # åˆ›å»º DuckDB è¿æ¥
    con = duckdb.connect()

    # ä½¿ç”¨ DuckDB è¯»å–åŸå§‹ Parquet æ–‡ä»¶ï¼Œå¹¶åˆ›å»ºè§†å›¾ v_hit
    con.execute(f"CREATE VIEW v_hit AS SELECT * FROM read_parquet('{input_file}')")

    # å°†å‰ rows_limit è¡Œæ•°æ®å†™å…¥æ–°çš„ Parquet æ–‡ä»¶
    con.execute(f"""
        COPY (
            SELECT * FROM v_hit
            LIMIT {rows_limit}  -- é™åˆ¶æ¯æ¬¡å†™å…¥çš„è¡Œæ•°
        ) TO '{output_file}' (FORMAT PARQUET)  -- å†™å…¥åˆ°è¾“å‡ºæ–‡ä»¶
    """)

    # å…³é—­è¿æ¥
    con.close()
    print(f"[+] å†™å…¥å®Œæˆ: {output_file}")


# ä¼°ç®—æ¯è¡Œçš„å¤§å°ï¼Œå¹¶è®¡ç®—æ–‡ä»¶çš„æ€»è¡Œæ•°
def estimate_avg_row_size(input_file):
    """
    ä¼°ç®—æ¯è¡Œçš„å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œå¹¶è®¡ç®— Parquet æ–‡ä»¶çš„æ€»è¡Œæ•°ã€‚
    """
    con = duckdb.connect()

    # åˆ›å»ºä¸€ä¸ª DuckDB è§†å›¾ï¼Œç›´æ¥è¯»å–è¾“å…¥çš„ Parquet æ–‡ä»¶
    con.execute(f"CREATE VIEW v_hit AS SELECT * FROM read_parquet('{input_file}')")

    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶æ¥ä¿å­˜æ•°æ®æ ·æœ¬ï¼Œç”¨äºè®¡ç®—å¹³å‡è¡Œå¤§å°
    tmp_file = "tmp_sample.parquet"
    if os.path.exists(tmp_file):
        os.remove(tmp_file)

    # é€‰æ‹©å‰ 10,000 è¡Œä½œä¸ºæ ·æœ¬å¹¶å°†å®ƒä»¬å†™å…¥ä¸´æ—¶æ–‡ä»¶
    con.execute("COPY (SELECT * FROM v_hit LIMIT 10000) TO 'tmp_sample.parquet' (FORMAT PARQUET)")

    # è·å–ä¸´æ—¶æ–‡ä»¶çš„å¤§å°ï¼Œå¹¶è®¡ç®—æ¯è¡Œçš„å¹³å‡å¤§å°
    file_size = os.path.getsize(tmp_file)
    avg_row_size = file_size / 10000

    # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    os.remove(tmp_file)

    # è·å–æ–‡ä»¶ä¸­çš„æ€»è¡Œæ•°
    num_rows = con.execute("SELECT COUNT(*) FROM v_hit").fetchone()[0]

    con.close()
    return avg_row_size, num_rows


# åˆå¹¶åˆ†ç‰‡å¹¶åˆ é™¤å·²å¤„ç†çš„æ–‡ä»¶
def merge_parquet_files_streaming(input_dir, output_file):
    """
    å°†å¤šä¸ªåˆ†ç‰‡æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ªå¤§çš„ Parquet æ–‡ä»¶ï¼Œåˆå¹¶è¿‡ç¨‹ä¸­å®æ—¶åˆ é™¤å·²ç»å¤„ç†çš„æ–‡ä»¶ã€‚
    """
    print(f"ğŸ” è¾¹åˆå¹¶è¾¹åˆ é™¤ï¼Œç›®æ ‡è¾“å‡ºæ–‡ä»¶ï¼š{output_file}")

    # è·å–æ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶ï¼Œå¹¶æŒ‰åç§°æ’åº
    files = sorted([
        os.path.join(input_dir, f)
        for f in os.listdir(input_dir)
        if f.startswith("hit_part_") and f.endswith(".parquet")
    ])

    # å®šä¹‰ä¸€ä¸ª writer å˜é‡ï¼Œç”¨äºåœ¨å†™å…¥è¿‡ç¨‹ä¸­ä¿æŒæ–‡ä»¶å¥æŸ„
    writer = None

    # å¤„ç†æ¯ä¸€ä¸ªåˆ†ç‰‡æ–‡ä»¶
    for f in files:
        print(f"ğŸ“¥ è¯»å–å¹¶å†™å…¥: {f}")
        pf = pq.ParquetFile(f)
        # é€æ‰¹è¯»å–æ•°æ®å¹¶å†™å…¥ç›®æ ‡æ–‡ä»¶
        for batch in pf.iter_batches(batch_size=1_000_000):  # ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡
            table = pa.Table.from_batches([batch])
            # å¦‚æœ writer ä¸º Noneï¼Œè¯´æ˜æ˜¯ç¬¬ä¸€æ¬¡å†™å…¥ï¼Œåˆå§‹åŒ– writer
            if writer is None:
                writer = pq.ParquetWriter(output_file, table.schema)
            writer.write_table(table)

        # åˆ é™¤å·²å¤„ç†çš„åˆ†ç‰‡æ–‡ä»¶ï¼Œé‡Šæ”¾ç©ºé—´
        os.remove(f)
        print(f"ğŸ—‘ï¸ å·²åˆ é™¤: {f}")

    # å®Œæˆåˆå¹¶åå…³é—­ writer
    if writer:
        writer.close()
        print(f"âœ… åˆå¹¶å®Œæˆ: {output_file}")
    else:
        print("âš ï¸ æœªå‘ç°ä»»ä½• parquet åˆ†ç‰‡æ–‡ä»¶å¯åˆå¹¶")


# æ‰©å±• Parquet æ–‡ä»¶å¹¶å°†æ•°æ®å†™å…¥å¤šä¸ªåˆ†ç‰‡
def extend_parquet_parallel(input_file, output_dir, target_size_gb, num_workers=4):
    """
    æ‰©å±•åŸå§‹ Parquet æ–‡ä»¶ï¼Œç”Ÿæˆå¤šä¸ªåˆ†ç‰‡å¹¶åˆå¹¶æˆç›®æ ‡å¤§å°ã€‚
    ä½¿ç”¨å¹¶å‘å¤„ç†æ¥åŠ é€Ÿè¿‡ç¨‹ã€‚
    """
    # ä¼°ç®—æ¯è¡Œå¤§å°å¹¶è·å–æ–‡ä»¶æ€»è¡Œæ•°
    avg_row_size, num_rows = estimate_avg_row_size(input_file)

    # è®¡ç®—ç›®æ ‡æ–‡ä»¶çš„å­—èŠ‚å¤§å°å’Œæ¯ä¸ªåˆ†ç‰‡çš„å¤§å°
    total_bytes = target_size_gb * 1024 ** 3
    per_file_bytes = 256 * 1024 ** 2  # æ¯ä¸ªåˆ†ç‰‡å¤§å°ä¸º 256MBï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    rows_per_part = int(per_file_bytes / avg_row_size)  # æ¯ä¸ªåˆ†ç‰‡åº”åŒ…å«çš„è¡Œæ•°
    total_parts = int(total_bytes / per_file_bytes) + 1  # æ€»å…±éœ€è¦å¤šå°‘ä¸ªåˆ†ç‰‡

    # è¾“å‡ºä¼°ç®—ä¿¡æ¯
    print(f"ä¼°ç®—å¹³å‡è¡Œå¤§å°: {avg_row_size:.2f} å­—èŠ‚")
    print(f"æ¯åˆ†ç‰‡çº¦ {per_file_bytes / 1024**2:.2f} MBï¼Œçº¦ {rows_per_part:,} è¡Œ")
    print(f"ç›®æ ‡å¤§å°: {target_size_gb}GBï¼Œéœ€ç”Ÿæˆ {total_parts} åˆ†ç‰‡")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)

    # å‡†å¤‡å¤šä¸ªä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡å†™å…¥ä¸€ä¸ªåˆ†ç‰‡
    file_args = []
    for i in range(total_parts):
        output_file = os.path.join(output_dir, f"hit_part_{i:03d}.parquet")
        file_args.append((input_file, output_file, rows_per_part))

    # ä½¿ç”¨å¹¶å‘æ± æ¥åŠ é€Ÿåˆ†ç‰‡å†™å…¥è¿‡ç¨‹
    print(f"ğŸš€ å¹¶å‘å†™å…¥ä¸­ï¼ˆ{num_workers} å¹¶å‘ï¼‰...")
    with multiprocessing.Pool(processes=num_workers) as pool:
        pool.starmap(write_partition_with_limit, file_args)

    print("âœ… æ‰€æœ‰åˆ†æ–‡ä»¶å†™å…¥å®Œæˆ")

    # åˆå¹¶æ‰€æœ‰åˆ†ç‰‡å¹¶è¾“å‡ºæœ€ç»ˆæ–‡ä»¶
    final_file = os.path.join(output_dir, f"hits_{target_size_gb}gb.parquet")
    merge_parquet_files_streaming(output_dir, final_file)

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="æ‰©å±• ClickBench hit.parquet æ•°æ®é›†")
    parser.add_argument("--input", help="è¾“å…¥åŸå§‹ hit.parquet æ–‡ä»¶è·¯å¾„", default="/home/zsl/datafusion-ballista/benchmarks/clickbench_data/hits.parquet")
    parser.add_argument("--output_dir", help="è¾“å‡ºç›®å½•", default="/home/zsl/datafusion-ballista/benchmarks/clickbench_data")
    parser.add_argument("--size_gb", type=int, help="ç›®æ ‡å¤§å°ï¼ˆå•ä½ GBï¼‰", default=50)
    parser.add_argument("--workers", type=int, default=6, help="å¹¶å‘å†™å…¥æ•°é‡")

    args = parser.parse_args()

    extend_parquet_parallel(
        input_file=args.input,
        output_dir=args.output_dir,
        target_size_gb=args.size_gb,
        num_workers=args.workers
    )
